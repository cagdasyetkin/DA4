---
title: "CEU Data Analysis 4"
author: "Cagdas Yetkin"
date: '2018-02-11'
output:
  html_notebook:
    df_print: paged
  word_document: default
  pdf_document: default
  html_document:
    df_print: paged
subtitle: Assignment 1
---

![text text text](londonMap.png)

In this task we are going to predict ..............


```{r message=FALSE, warning=FALSE}

library(doParallel)
library(ggplot2)
library(data.table)
library(caret)
library(glmnet)
library(ROCR)
library(dplyr)
library(ggthemes)
library(skimr)
library(gridExtra)
library(stargazer)

registerDoParallel(cores = 4)

```

## 1. 

```{r}

data <- fread("airbnb_london_workfile.csv", stringsAsFactors = TRUE)

data[, c("neighbourhood_cleansed", "cancellation_policy", 
         "price", "property_type", "room_type")  := NULL]

data[, neighbourhood_cleansed := NULL]

names(data)
data[, usd_price_day := as.numeric(usd_price_day)]

```

Explore 
```{r}
glimpse(data)
```

I tried PCA exploration to reduce dimentions but it was not useful: 80% of the variation was captured by 50 variables.


convert dummies to factor

```{r}
#dummies <- names(data)[grep("^d_.*",names(data))]

#setDT(data)[, (dummies):= lapply(.SD, factor), .SDcols=dummies]

```

```{r}
sapply(data, function(x) sum(is.na(x)))
```

I have 6-7 observations with strange prices such as 1, 2, 5 usd. I will drop them.

```{r}
data <- data[data$usd_price_day > 10, ]
```


```{r}
data[, usd_cleaning_fee := 
       ifelse(is.na(usd_cleaning_fee), 0, usd_cleaning_fee)]
skim(data$usd_cleaning_fee)
```
```{r}
data[, n_bathrooms := 
       ifelse(is.na(n_bathrooms), median(n_bathrooms, na.rm = T), n_bathrooms)]
skim(data$n_bathrooms)
```

```{r}
data[, n_beds := 
       ifelse(is.na(n_beds), n_accommodates, n_beds)]
skim(data$n_beds)
```

dont impute the other NAs.

study your variables:


```{r}
sapply(data, function(x) if (class(x) == 'factor') {unique(x)
  } else if (class(x) != 'factor') {summary(x)})
```
How does my price behave? Are my missing observations missing at random?

```{r}
#convert to numeric
#Transform the variable n_accommodates to numeric (in order to capture the order of the intervals)
data <- data[, n_accommodates := as.numeric(as.character(n_accommodates))]
```

```{r}

plot1 <- ggplot(data, aes(x = factor(n_accommodates), y = usd_price_day, fill = factor(f_property_type))) +
         geom_boxplot(alpha=0.8) +
         scale_x_discrete(name = "Accomodate Persons") +
         ggtitle("Price Per Accommodate") + theme_fivethirtyeight()

plot2 <- ggplot(data = data) +
         geom_bar(data = data,
         aes(x = factor(n_accommodates),
         color = f_room_type, fill = f_room_type)) +
         ggtitle("Accomodations and property types") +
         xlab('Accommodates') + theme_fivethirtyeight()

grid.arrange(plot1, plot2, nrow=2)
```

```{r}
## Distribution of price by room type
plot3 <- ggplot(data, aes(usd_price_day, fill = f_room_type)) + 
         ggtitle("Density, Price by Type") +
         geom_density(alpha = 0.3) + theme_fivethirtyeight()

## Boxplot of price by room type
plot4 <- ggplot(data, aes(f_room_type, usd_price_day)) + 
         ggtitle("Price by Type") +
         geom_boxplot()+ theme_fivethirtyeight()

grid.arrange(plot3, plot4, ncol=2)
```
These room types are totally different animals. Lets look at their boxplots.

```{r}
skim(data$usd_price_day)
```

now see the missing observations

```{r}
## Distribution of price by room type for missing reviews
plot5 <- ggplot(data[is.na(n_reviews_per_month)], aes(usd_price_day, fill = f_room_type)) + 
         ggtitle("Missing Review") +
         geom_density(alpha = 0.3) + theme_fivethirtyeight()

## Boxplot of price by room type
plot6 <- ggplot(data[is.na(n_reviews_per_month)], aes(f_room_type, usd_price_day)) + 
         ggtitle("Missing Review") +
         geom_boxplot()+ theme_fivethirtyeight()

grid.arrange(plot5, plot6, ncol=2)
```

```{r}
## Distribution of price by room type for missing reviews
skim(data[is.na(n_reviews_per_month), ]$usd_price_day)
```

```{r}
## Distribution of price by room type for missing reviews
plot7 <- ggplot(data[is.na(n_review_scores_rating)], aes(usd_price_day, fill = f_room_type)) + 
         ggtitle("Missing Review Score") +
         geom_density(alpha = 0.3) + theme_fivethirtyeight()

## Boxplot of price by room type
plot8 <- ggplot(data[is.na(n_review_scores_rating)], aes(f_room_type, usd_price_day)) + 
         ggtitle("Missing Review Score") +
         geom_boxplot()+ theme_fivethirtyeight()

grid.arrange(plot7, plot8, ncol=2)
```

```{r}
## Distribution of price by room type for it
skim(data[is.na(n_review_scores_rating), ]$usd_price_day)
```

```{r}
## Distribution of price by room type for host response rate
skim(data[is.na(p_host_response_rate), ]$usd_price_day)
```

Missing values seem to be missing at random. However, I will tabulate the missing values further and see if they are missing at random:

```{r, include=FALSE}
data[is.na(n_review_scores_rating), .(Tally = .N,
                                      AvgPrc = mean(usd_price_day)),
     
     keyby = f_neighbourhood_cleansed][order(-Tally)]
  
```


```{r, include=FALSE}
data[!is.na(n_review_scores_rating), .(Tally = .N,
                                      AvgPrc = mean(usd_price_day)),
     
     keyby = f_neighbourhood_cleansed][order(-Tally)]
  
```

```{r, include=FALSE}
data[is.na(n_review_scores_rating), .(Tally = .N,
                                      AvgPrc = mean(usd_price_day)),
     
     keyby = f_room_type][order(-Tally)]
```

```{r, include=FALSE}
data[!is.na(n_review_scores_rating), .(Tally = .N,
                                      AvgPrc = mean(usd_price_day)),
     
     keyby = f_room_type][order(-Tally)]
```

```{r, include=FALSE}
data[is.na(n_review_scores_rating), .(Tally = .N,
                                      AvgPrc = mean(usd_price_day)),
     
     keyby = f_property_type][order(-Tally)]
```
```{r, include=FALSE}
data[!is.na(n_review_scores_rating), .(Tally = .N,
                                      AvgPrc = mean(usd_price_day)),
     
     keyby = f_property_type][order(-Tally)]
```

```{r, include=FALSE}
data[is.na(n_review_scores_rating), .(Tally = .N,
                                      AvgPrc = mean(usd_price_day)),
     
     keyby = n_accommodates][order(-Tally)]
```
```{r, include=FALSE}
data[!is.na(n_review_scores_rating), .(Tally = .N,
                                      AvgPrc = mean(usd_price_day)),
     
     keyby = n_accommodates][order(-Tally)]
```



We can see that missing observations for these variables are following the overall price distribution.
We will drop these values with missing observation.

```{r}
data <- data %>%
          filter(!is.na(p_host_response_rate)) %>%
          filter(!is.na(n_review_scores_rating)) %>%
          filter(!is.na(n_reviews_per_month)) %>%
          filter(!is.na(n_days_since))

data <- data[complete.cases(data),]

```


Now time to forget about entire London and select a random district...


```{r}
#a dplyr way...
set.seed(22) #reproduce
selection_table <- data %>%
                      group_by(f_neighbourhood_cleansed) %>%
                      tally() %>%
                      filter(n > 1000) %>%
                      arrange(desc(n))


lottery_winner <- as.vector(sample(selection_table$f_neighbourhood_cleansed, 1))
lottery_winner
```

Hello My District...

```{r}
district <- data %>%
              filter(f_neighbourhood_cleansed == lottery_winner)

```

```{r}
## Distribution of price by room type
ggplot(district, aes(usd_price_day, fill = f_room_type)) + 
  geom_density(alpha = 0.3) + theme_fivethirtyeight()
```
Its a kind of similar to entire London actually.

```{r}
skim(district$usd_price_day)
```



```{r}
ggplot(district, aes(x = factor(n_accommodates), y = usd_price_day, fill = factor(f_property_type))) +
        geom_boxplot(alpha=0.8) +
        scale_x_discrete(name = "Accomodate Persons") +
        ggtitle("Price per Accoomodate") + theme_fivethirtyeight()
```
```{r}
names(district)
```

These prices are not log normal. I will not use log price.

```{r, fig.width=5,fig.height=3}
## Distribution of salaries
#ggplot(district, aes(usd_price_day)) + geom_histogram()
#ggplot(district, aes(log(usd_price_day))) + geom_histogram()
#ggplot(district, aes(usd_price_day, usd_cleaning_fee)) + geom_point()
#ggplot(district, aes(p_host_response_rate, usd_price_day)) + geom_point()
#ggplot(district, aes(n_accommodates, usd_price_day)) + geom_point() + geom_jitter()
#ggplot(district, aes(n_bathrooms, usd_price_day)) + geom_point()+ geom_jitter()
#ggplot(district, aes(n_review_scores_rating, usd_price_day)) + geom_point()
#ggplot(district, aes(n_number_of_reviews, usd_price_day)) + geom_point()

```

```{r, fig.width=5,fig.height=2, message=FALSE, warning=FALSE}
#The most importand dummy variables will be plotted this way
#https://sakaluk.wordpress.com/2015/08/27/6-make-it-pretty-plotting-2-way-interactions-with-ggplot2/

library(psych)
dist_stats <- describeBy(district$usd_price_day,list(district$f_room_type,district$d_familykidfriendly), mat=TRUE,digits=2)

dist_stats$se = dist_stats$sd/sqrt(dist_stats$n)

names(dist_stats)[names(dist_stats) == 'group1'] = 'Type'
names(dist_stats)[names(dist_stats) == 'group2'] = 'KidFriendly'


limits = aes(ymax = mean + (1.96*se), ymin=mean - (1.96*se))
dodge = position_dodge(width=0.9)

apatheme=theme_bw()+
  theme(panel.grid.major=element_blank(),
        panel.grid.minor=element_blank(),
        panel.border=element_blank(),
        axis.line=element_line())

p1 <- ggplot(dist_stats, aes(x = Type, y = mean, fill = KidFriendly))+
  geom_bar(stat='identity', position=dodge)+
  geom_errorbar(limits, position=dodge, width=0.25)+
  ylab('Mean Price')+
  theme_fivethirtyeight() + apatheme +
  scale_fill_grey()
p1

#library(gridExtra)
#plot1 <- qplot(iris$Sepal.Length)
#plot2 <- qplot(iris$Sepal.Width)
#grid.arrange(plot1, plot2, ncol=2)

```

```{r}
london <- data
```

Run a Random Forest to entire London to see what it shows

```{r}
# Fit random forest: model

RFtrainControl <- trainControl(method = "cv", 
                               number = 3, 
                               verboseIter = TRUE)
set.seed(1234)
RFmodel <- train(
  usd_price_day ~ .,
  tuneLength = 2,
  data = district, 
  method = 'ranger',
  #tuneGrid = data.frame(mtry = c(2, 20, 40, 60)),
  importance = 'impurity',
  trControl = RFtrainControl)

RFmodel
```

```{r}
names(RFmodel$modelInfo)
varImp(RFmodel)
```

These seem to be my most important variables. I will try them in my linear models.


Partitioning the data to 70% training and 30% test samples.


```{r}

my_ratio <- 0.7

#First for the entire London
set.seed(123)
train_indices_london <- createDataPartition(y = london[["usd_price_day"]],
                                     times = 1,
                                     p = my_ratio,
                                     list = FALSE)

data_train_london <- london[train_indices_london, ]
data_test_london  <- london[-train_indices_london, ]

#Now for the selected district
set.seed(123)
train_indices_district <- createDataPartition(y = district[["usd_price_day"]],
                                     times = 1,
                                     p = my_ratio,
                                     list = FALSE)

data_train_district <- district[train_indices_district, ]
data_test_district  <- district[-train_indices_district, ]

```

```{r}
train_control <- trainControl(method = "cv", number = 10)
#tune_grid <- data.frame("cp" = c(0.01, 0.001, 0.0001, 0.00001))
```

```{r}
# Linear 1

set.seed(1234) 
model_1_london <- train(usd_price_day ~ n_accommodates, 
                   data = data_train_london, 
                   method = "lm", 
                   trControl = train_control)
model_1_london$results[["RMSE"]]

set.seed(1234) 
model_1_district <- train(usd_price_day ~ n_accommodates, 
                   data = data_train_district, 
                   method = "lm", 
                   trControl = train_control)
#model_1_district$results[["RMSE"]]

```

```{r}
resamps <- resamples(list("model_1_london" = model_1_london,
                          "model_1_district" = model_1_district))
summary(resamps)

```


they are performing almost the same on test sets:

```{r}
#postResample(data_test_district$usd_price_day, predict(model_1_district, data_test_district))
#postResample(data_test_london$usd_price_day, predict(model_1_london, data_test_london))
test_prediction_district <- predict.train(model_1_district, newdata = data_test_district)

test_prediction_london <- predict.train(model_1_london, newdata = data_test_london)

cat('District RMSE: ', RMSE(test_prediction_district, data_test_district[["usd_price_day"]]),
    'London RMSE: ',   RMSE(test_prediction_london, data_test_london[["usd_price_day"]]))


```


```{r}
# Linear 2

#sapply(data_train_london, function(x) sum(is.na(x)))


set.seed(1234) 
model_2_london <- train(usd_price_day ~ n_accommodates + f_room_type + n_beds + n_bathrooms +
                          f_property_type + f_bed_type + d_familykidfriendly, 
                   data = data_train_london, 
                   method = "lm", 
                   trControl = train_control)
                   #preProcess=c("center", "scale"))
#model_1_london$results[["RMSE"]]

set.seed(1234)
model_2_district <- train(usd_price_day ~ n_accommodates + f_room_type + n_beds + n_bathrooms +
                          f_property_type + f_bed_type + d_familykidfriendly, 
                   data = data_train_district,
                   method = "lm", 
                   trControl = train_control)
                   #preProcess=c("center", "scale"))
#model_1_district$results[["RMSE"]]

```


```{r}
resamps <- resamples(list("model_2_london" = model_2_london,
                          "model_2_district" = model_2_district))
summary(resamps)

```

they are performing almost the same on test sets:

```{r}
#postResample(data_test_district$usd_price_day, predict(model_1_district, data_test_district))
#postResample(data_test_london$usd_price_day, predict(model_1_london, data_test_london))
test_prediction_district <- predict.train(model_2_district, newdata = data_test_district)

test_prediction_london <- predict.train(model_2_london, newdata = data_test_london)

cat('District RMSE: ', RMSE(test_prediction_district, data_test_district[["usd_price_day"]]),
    'London RMSE: ',   RMSE(test_prediction_london, data_test_london[["usd_price_day"]]))


```


```{r}
# Linear 3

# Factor values
#X1  <- c("f_room_type*f_property_type",  "n_number_of_reviews*f_property_type") 
# Interactions of factors and dummies
#X2  <- c("d_airconditioning*f_property_type", "d_cats*f_property_type", "d_dogs*f_property_type") 

set.seed(1234) 
model_3_london <- train(usd_price_day ~ n_accommodates + f_room_type + n_beds + n_bathrooms +
                          f_property_type + f_bed_type + d_familykidfriendly + 
                          f_room_type*f_property_type +  n_number_of_reviews*f_property_type +
                          d_airconditioning*f_property_type + 
                          d_cats*f_property_type + d_dogs*f_property_type +
                          d_familykidfriendly*f_property_type,
                   data = data_train_london, 
                   method = "lm", 
                   trControl = train_control)
                   #preProcess=c("center", "scale"))
#model_1_london$results[["RMSE"]]

set.seed(1234)
model_3_district <- train(usd_price_day ~ n_accommodates + f_room_type + n_beds + n_bathrooms +
                          f_property_type + f_bed_type + d_familykidfriendly + 
                          f_room_type*f_property_type +  n_number_of_reviews*f_property_type +
                          d_airconditioning*f_property_type + 
                          d_cats*f_property_type + d_dogs*f_property_type +
                          d_familykidfriendly*f_property_type,
                   data = data_train_district,
                   method = "lm", 
                   trControl = train_control)
                   #preProcess=c("center", "scale"))
#model_1_district$results[["RMSE"]]

```

```{r}
resamps <- resamples(list("model_3_london" = model_3_london,
                          "model_3_district" = model_3_district))
summary(resamps)

```

they are performing almost the same on test sets:

```{r}
#postResample(data_test_district$usd_price_day, predict(model_1_district, data_test_district))
#postResample(data_test_london$usd_price_day, predict(model_1_london, data_test_london))
test_prediction_district <- predict.train(model_3_district, newdata = data_test_district)

test_prediction_london <- predict.train(model_3_london, newdata = data_test_london)

cat('District RMSE: ', RMSE(test_prediction_district, data_test_district[["usd_price_day"]]),
    'London RMSE: ',   RMSE(test_prediction_london, data_test_london[["usd_price_day"]]))


```



```{r}
# Linear 4
#-p_host_response_rate -n_review_scores_rating -n_reviews_per_month -n_days_since -d_washerdryer -d_freeparkingonstreet -d_paidparkingoffpremises

set.seed(1234) 
model_4_london <- train(usd_price_day ~ .,
                          
                   data = data_train_london, 
                   method = "lm", 
                   trControl = train_control)
                   #preProcess=c("center", "scale"))
#model_1_london$results[["RMSE"]]

set.seed(1234)
model_4_district <- train(usd_price_day ~ .,
                   data = data_train_district,
                   method = "lm", 
                   trControl = train_control)
                   #preProcess=c("center", "scale"))
#model_1_district$results[["RMSE"]]

```

```{r}
resamps <- resamples(list("model_4_london" = model_4_london,
                          "model_4_district" = model_4_district))
summary(resamps)

```

they are performing almost the same on test sets:

```{r}
#postResample(data_test_district$usd_price_day, predict(model_1_district, data_test_district))
#postResample(data_test_london$usd_price_day, predict(model_1_london, data_test_london))
test_prediction_district <- predict.train(model_4_district, newdata = data_test_district)

test_prediction_london <- predict.train(model_4_london, newdata = data_test_london)

cat('District RMSE: ', RMSE(test_prediction_district, data_test_district[["usd_price_day"]]),
    'London RMSE: ',   RMSE(test_prediction_london, data_test_london[["usd_price_day"]]))


```




############### Code From Machine Learning Class ##################################

```{r}
data[, age_cat := cut(age, .(-Inf,20,35,65,Inf), labels = c('Kids','Young','Middle','Old'))]
```

```{r}
ggplot(data, aes(age_cat)) + geom_bar() + facet_grid(~gender) + theme_economist_white()
```
This data is mostly about young males which is representative for tech industry. But which one is seeking relatively more treatment? Males or females?

```{r}
ggplot(data, aes(x = treatment, group = gender)) + 
  geom_bar(aes(y = ..prop.., fill = factor(..x..)), stat="count") +
  geom_text(aes( label = scales::percent(..prop..),
                   y= ..prop.. ), stat= "count", vjust = -.1) +
  scale_y_continuous(labels=scales::percent) +
  facet_grid(~gender) +
  labs(y = "Percent", fill="Treatment") +
  theme_fivethirtyeight() + scale_fill_grey()
  
```

I said males or females and the trans gender came out with a surprise

How about family history?

```{r}
ggplot(data, aes(x = treatment, group = family_history)) + 
  geom_bar(aes(y = ..prop.., fill = factor(..x..)), stat="count") +
    geom_text(aes( label = scales::percent(..prop..),
                   y= ..prop.. ), stat= "count", vjust = -.1) +
  scale_y_continuous(labels=scales::percent) +
  facet_grid(~family_history) +
  labs(y = "Percent", fill="Treatment") +
  theme_fivethirtyeight() + scale_fill_grey()
```

Family really matters a big deal!


```{r}
ggplot(data, aes(x = treatment, group = age_cat)) + 
  geom_bar(aes(y = ..prop.., fill = factor(..x..)), stat="count") +
    geom_text(aes( label = scales::percent(..prop..),
                   y= ..prop.. ), stat= "count", vjust = -.1) +
  scale_y_continuous(labels=scales::percent) +
  facet_grid(~age_cat) +
  labs(y = "Percent", fill="Treatment") +
  theme_fivethirtyeight() + scale_fill_grey()

```

We can see a pattern here. Older the age category, more the treatment. Similarly younger age categories are seeking less and less treatment.

Why do I have 100% in the oldest age category?
```{r}
data %>% filter(age > 65) %>% select(age_cat, gender, family_history, treatment)
```
It turns out there are only 3 observations over there. As we have seen above the dominant age category is 20-35 years old.


```{r}
ggplot(data, aes(x = treatment, group = seek_help)) + 
  geom_bar(aes(y = ..prop.., fill = factor(..x..)), stat="count") +
  geom_text(aes( label = scales::percent(..prop..),
                   y= ..prop.. ), stat= "count", vjust = -.1) +
  labs(y = "Percent", fill="treatment") +
  facet_grid(~seek_help) +
  scale_y_continuous(labels = scales::percent) +
  theme_fivethirtyeight() + scale_fill_grey()

```

There can be some evidence for the higher probability of ending up in a treatment for the people who are aware of their employer's services regarding mental issues


```{r}
ggplot(data, aes(x = treatment, group = leave)) + 
  geom_bar(aes(y = ..prop.., fill = factor(..x..)), stat="count") +
  geom_text(aes( label = scales::percent(..prop..),
                   y= ..prop.. ), stat= "count", vjust = -.2) +
  labs(y = "Percent", fill="treatment") +
  facet_grid(~leave) +
  scale_y_continuous(labels = scales::percent) +
  theme_fivethirtyeight() + scale_fill_grey()


```

People who indicate that there is some difficulty on getting a leave for mental issue reasons might giving us signal.



Partitioning the data to 70% training and 30% test samples.


```{r}
set.seed(123)
my_ratio <- 0.7
train_indices <- createDataPartition(y = data[["treatment"]],
                                     times = 1,
                                     p = my_ratio,
                                     list = FALSE)

data_train <- data[train_indices, ]
data_test  <- data[-train_indices, ]

```


Build models with glmnet and rpart that predict the binary outcome of treatment. Using cross-validation on the training set and AUC as a selection measure.


```{r}
train_control <- trainControl(method = "cv",
                              number = 5,
                              classProbs = TRUE,
                              summaryFunction = twoClassSummary)

tune_grid <- expand.grid("alpha" = c(0, 1),
                         "lambda" = seq(0.1, 0.14, 0.01))

set.seed(123)
glmnet_model <- train(treatment ~ age_cat + seek_help + benefits + supervisor +
                                  family_history + leave + gender,
                      data = data_train,
                      method = "glmnet",
                      preProcess = c("center", "scale"),
                      trControl = train_control,
                      tuneGrid = tune_grid,
                      metric = "ROC") 
glmnet_model
```

```{r}

trctrl <- trainControl(method = "cv", 
                       number = 5, 
                       classProbs = T, 
                       #verboseIter = T, 
                       summaryFunction = twoClassSummary)

tune_grid <- data.frame(cp=seq(0.0001, 0.01, 0.001))

set.seed(123)
treeCPModel <- train(treatment ~ age_cat + seek_help + benefits + supervisor +
                                 family_history + leave + gender,
                     data = data_train, 
                     method = "rpart",
                     trControl = trctrl,
                     preProcess = c("center", "scale"),
                     tuneGrid = tune_grid,
                     metric = 'ROC')
                  
treeCPModel
```

Compare models based on their predictive performance based on the cross-validation information, we can just use the mean AUC to select the best model. 

###Glmnet performs better than the tree: 74.5% vs 71.9% area under the curve.

Now evaluating the better performing model on the test set, drawing a ROC curve and interpreting the AUC.

```{r}
test_prediction <- predict.train(glmnet_model, newdata = data_test)
test_truth <- data_test[["treatment"]]
```

There are 4 cases:

* true positives: those that are positive in reality and we correctly predict
them to be positive
* false positives: those that are negative in reality and we falsely predict
them to be positive
* true negatives: those that are negative in reality and we correctly predict
them to be negative
* false negatives: those that are positive in reality and we falsely predict
them to be negative

```{r}
confusionMatrix(test_prediction, test_truth)
```

The various types of errors have to be examined and we have to decide
based on them.

```{r}
# obtain probabilities instead of binary predictions
test_prediction_probs <- predict.train(glmnet_model, 
                                       newdata = data_test, 
                                       type = "prob")
head(test_prediction_probs)
```
```{r}
summary(test_prediction_probs$Yes)
```

By default, predict.train uses the 50% threshold for prediction

```{r}
test_prediction_v2 <- ifelse(test_prediction_probs$Yes > 0.45, "Yes", "No")
test_prediction_v2 <- factor(test_prediction_v2, levels = c("Yes", "No"))
confusionMatrix(test_prediction_v2, test_truth)
```

We lowered our false negatives a bit by using the threshold of 0.45 above. Lets do a search for threshold.

###Varying thresholds

If we increase the threshold for predicting something to be positive:
we will have less and less cases that we label as positive. Both of those
that are positive in reality and of those that are negative. Thus, both
the true positives and the false positives increase.

```{r}
thresholds <- seq(0.3, 0.6, by = 0.05)

for (thr in thresholds) {
  test_prediction <- ifelse(test_prediction_probs$Yes > thr, "Yes", "No")
  test_prediction <- factor(test_prediction, levels = c("Yes", "No"))
  print(paste("Threshold:", thr))
  print(confusionMatrix(test_prediction, test_truth)[["table"]])
} 
```

What to choose then? I would choose .45 threshold because it is a balanced point for my business objectives.

My strategy is to minimize the situations where I predict no-treatment but in reality it is a yes. In those cases I do a terrible mistake. The outcome has a high cost for the company. I am not aware of the troubled people around. It is like sitting on a time bomb which we dont know it exists.

However, I cant lower my threshold too much also. In that case I would do too many false positives which has other kind of costs. It will impact the time and productivity negatively.

On the other hand, we can accept False Positives until to a centain extend. There is a sweet spot where they are not much costly compared to False Negatives. Just like some false fire alarms will not cause a lot of trouble.

After some scientific meditation, 0.45 will be the threshold I would use.

At 0.45 Threshold Point I have:
<p>a) 78.9% True Positive Rate. That is TP/(TP+FN) >> 150/(150+40)</p>

<p>b) 44% False Positive Rate. That is FP/(FP+TN) >> 82/(82+104)</p>

82 False Posities and 40 False Negatives.

### Lets see The ROC curve

The ROC curve summarizes how a binary classifier performs "overall", taking
into accounts all possible thresholds. It shows the trade-off 
between true positive rate (a.k.a sensitivity, # true positives / 
# all positives) and the false positive rate (a.k.a 1 - specificity, 
# false positive / # negatives).

```{r}
# a ggplot
# using prediction function from ROCR package
glmnet_prediction <- prediction(test_prediction_probs$Yes,
                              data_test[["treatment"]])
glmnet_perf <- performance(glmnet_prediction, measure = "tpr", x.measure = "fpr")

glmnet_roc_df <- data.table(
  model = "glm",
  FPR = glmnet_perf@x.values[[1]],
  TPR = glmnet_perf@y.values[[1]],
  cutoff = glmnet_perf@alpha.values[[1]]
)

ggplot(glmnet_roc_df) +
  geom_line(aes(FPR, TPR, color = cutoff), size = 2) +
  geom_ribbon(aes(FPR, ymin = 0, ymax = TPR), alpha = 0.1) +
  geom_abline(intercept = 0, slope = 1,  linetype = "dotted", col = "black") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, .1)) +
  scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, .1)) +
  theme_fivethirtyeight() + scale_fill_grey() 
```

### AUC 

Higher AUC generally means better classification.

```{r}
# calculate AUC
AUC <- performance(glmnet_prediction, "auc")@y.values[[1]]
print(AUC)
```

The colored field's area in the plot is 0.725. A superb classifier would yield an area very close to 1. This is not one of the best models. However, it is better than flipping a coin. Or, it is better than a **dart throwing chimpanzee** 


## 2. Transformed scores
```{r}
# https://www.kaggle.com/joniarroba/noshowappointments
data <- fread("no-show-data.csv")
glimpse(data)
```

```{r}
# some data cleaning
data[, c("PatientId", "AppointmentID", "Neighbourhood") := NULL]
setnames(data, 
         c("No-show", 
           "Age", 
           "Gender",
           "ScheduledDay", 
           "AppointmentDay",
           "Scholarship",
           "Hipertension",
           "Diabetes",
           "Alcoholism",
           "Handcap",
           "SMS_received"), 
         c("no_show", 
           "age", 
           "gender", 
           "scheduled_day", 
           "appointment_day",
           "scholarship",
           "hypertension",
           "diabetes",
           "alcoholism",
           "handicap",
           "sms_received"))
# clean up a little bit
data <- data[age %between% c(0, 95)]
# for binary prediction with caret, the target variable must be a factor
data[, no_show := factor(no_show, levels = c("Yes", "No"))] #first one got to be YES, the positive
data[, no_show_num := ifelse(no_show == "Yes", 1, 0)]
data[, handicap := ifelse(handicap > 0, 1, 0)]

# create new variables
data[, scheduled_day := as.Date(scheduled_day)]
data[, appointment_day := as.Date(appointment_day)]
data[, days_since_scheduled := as.integer(appointment_day - scheduled_day)]
data <- data[days_since_scheduled > -1]
```


```{r}
data[, no_show_num := NULL]

data[, days_category := cut(
  days_since_scheduled, 
  breaks = c(-1, 0, 1, 2, 5, 10, 30, Inf), 
  include.lowest = TRUE)]

data[, age_category := cut(age, 
                           breaks = seq(0, 100, by = 5), 
                           include.lowest = TRUE)]
```

Now create a training and a test data and estimate a simple logistic regression
to predict `no_show`. 
```{r}
training_ratio <- 0.5 
set.seed(1234)
train_indices <- createDataPartition(y = data[["no_show"]],
                                     times = 1,
                                     p = training_ratio,
                                     list = FALSE)
data_train <- data[train_indices, ]
data_test <- data[-train_indices, ]
```


```{r, warning=FALSE}
train_control <- trainControl(method = "cv",
                              number = 5,
                              classProbs = TRUE)
set.seed(857)
glm_model <- train(no_show ~ age_category + gender + days_since_scheduled,
                   method = "glm",
                   data = data_train,
                   trControl = train_control)

test_prediction <- predict.train(glm_model, newdata = data_test)
test_truth <- data_test[["no_show"]]
```


```{r message=FALSE, warning=FALSE}
# obtain probabilities instead of binary predictions
test_prediction_probs <- predict.train(glm_model, 
                                       newdata = data_test, 
                                       type = "prob")
```
```{r message=FALSE, warning=FALSE}
summary(test_prediction_probs$Yes)
```


```{r message=FALSE, warning=FALSE}
prediction <- test_prediction_probs$Yes
prediction_sqrt <- sqrt(prediction)
prediction_sq <- prediction^2
```


```{r message=FALSE, warning=FALSE}
rocr_prediction <- prediction(prediction, test_truth)
rocr_sqrt <- prediction(prediction_sqrt, test_truth)
rocr_sq <- prediction(prediction_sq, test_truth)
# built-in plot method

plot(performance(rocr_sqrt, "tpr", "fpr"), colorize = FALSE) 
plot(performance(rocr_sq, "tpr", "fpr"), add = TRUE, colorize = FALSE) 
plot(performance(rocr_prediction, "tpr", "fpr"), add = TRUE, colorize = FALSE) 
```

Area under the curves are the same for all three of them. Because it is the same model. Sensitivity and specificity have an inverse relationship. Increasing one would always decrease the other and Area Under the Curve remains the same.

```{r}
# calculate AUC
AUC <- performance(rocr_prediction, "auc")@y.values[[1]]
AUC_sqrt <- performance(rocr_sqrt, "auc")@y.values[[1]]
AUC_sq <- performance(rocr_sq, "auc")@y.values[[1]]


AUC_results <- c(AUC, AUC_sqrt, AUC_sq)
AUC_results
```


### Calibration

Can the scores produced by the model be regarded as probabilities?
Let's calculate the predicted and actual share of positive cases for groups
of observations in the test set based on their predicted scores.

```{r}
truth_numeric <- ifelse(test_truth == "Yes", 1, 0)
score_glm <- test_prediction_probs$Yes

summary(score_glm)
```

```{r}
actual_vs_predicted <- data.table(actual = truth_numeric,
                                  predicted = score_glm)

actual_vs_predicted[, score_category := cut(predicted,
                                    seq(0, 0.4, 0.1),
                                    include.lowest = TRUE)]
calibration <- actual_vs_predicted[, .(mean_actual = mean(actual),
                                       mean_predicted = mean(predicted),
                                       num_obs = .N),
                                   keyby = .(score_category)]
ggplot(calibration,
       aes(x = mean_actual, y = mean_predicted, size = num_obs)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  ylim(0, 1) + xlim(0, 1)
```

This one is **well calibrated**. Predicted mean groups are similar to the actual means. It looks like a well calibrated classifier does not mean a perfect classifier.


Below we will see the calibration when we take squre root of the predictions. In general if we are following the 45 degree line it is a good sign for calibration.

```{r}
actual_vs_predicted <- data.table(actual = truth_numeric,
                                  predicted = prediction_sqrt)

actual_vs_predicted[, score_category := cut(predicted,
                                    seq(0, 0.4, 0.05),
                                    include.lowest = TRUE)]
calibration <- actual_vs_predicted[, .(mean_actual = mean(actual),
                                       mean_predicted = mean(predicted),
                                       num_obs = .N),
                                   keyby = .(score_category)]
ggplot(calibration,
       aes(x = mean_actual, y = mean_predicted, size = num_obs)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  ylim(0, 1) + xlim(0, 1)
```
Of course since we increased the values of our predictions they went up in the y axis, and the transformaton was not a linear one.

We will see a similar effect below to the **other** direction.

```{r}
actual_vs_predicted <- data.table(actual = truth_numeric,
                                  predicted = prediction_sq)

actual_vs_predicted[, score_category := cut(predicted,
                                    seq(0, 0.4, 0.05),
                                    include.lowest = TRUE)]
calibration <- actual_vs_predicted[, .(mean_actual = mean(actual),
                                       mean_predicted = mean(predicted),
                                       num_obs = .N),
                                   keyby = .(score_category)]
ggplot(calibration,
       aes(x = mean_actual, y = mean_predicted, size = num_obs)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  ylim(0, 1) + xlim(0, 1)
```
Nevertheless, even after perfect calibration of a classifier, its ROC is not affected and its classification ability remains unchanged. This is discussed in this paper in detail:
http://www.stat.wvu.edu/~jharner/courses/dsci503/docs/vuk.pdf

A good example about calibration is discussed also in Nate Silver's book the Signal and Noise regarding weather forecasts being tweaked a bit on tv channels to predict more rain. It is called the 'wet bias'. They want to predict more rain than the evidence suggests, not to make people angry.

Thank you!



